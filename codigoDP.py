# -*- coding: utf-8 -*-
"""Copia_de_Colorectal_histology_LeNet5_notebook(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pOFysVW1RAF1z6TWMoAWBfX6M3xxwVnj

# Colorectal histology dataset with LeNet 5

# Preview: Neural Network examples with `tf.keras`

- `tf.keras` Software (http://keras.io/);

- Visual example of LeNet-5 (http://yann.lecun.com/exdb/lenet/) using MNIST digits dataset;

- Examples with common neural network topologies (https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/).

## Colorectal histology

Methods to diagnose colorectal using histology images (<https://zenodo.org/record/53169#.XGZemKwzbmG>, <https://www.tensorflow.org/datasets/catalog/colorectal_histology>)

In this case, the purpose is to classify the type of histology in a given image in the following categories:

- 0: TUMOR
- 1: STROMA
- 2: COMPLEX
- 3: LYMPHO
- 4: DEBRIS
- 5: MUCOSA
- 6: ADIPOSE
- 7: EMPTY

## Local instalation (option 1)

Install the following Python packages to run this notebook

`pip install pip -U`

`pip install tensorflow jupyter`

## Google Colab (option 2)

[Google Colab](https://colab.research.google.com/) is a research project created to help disseminate machine learning education and research. It's a `Jupyter notebook` environment that requires no setup to use and runs entirely in the cloud.

Colaboratory notebooks are stored in [Google Drive](https://drive.google.com) and can be shared just as you would with Google Docs or Sheets. Colaboratory is free to use.

For more information, see our [FAQ](https://research.google.com/colaboratory/faq.html).

### How install extra packages
Google Colab installs a series of basic packages if we need any additional package just install it.
"""

!pip install scikit-learn -U
!pip install shap

"""## Import packages"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import sklearn
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold, KFold

import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow import keras
from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Define global constants

Lets start with a few epochs to test learning network parameters
"""

batch_size = 64
nb_classes = 2  #or 8
epochs = 50

# Scaling input image to theses dimensions
img_rows, img_cols = 32, 32
input_shape = (img_rows, img_cols, 3)

"""## Load image database"""

def format_example(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (img_rows, img_cols))
    return image


def load_data(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

"""## Plot images"""

def plot_symbols(X,y,n=15):
    index = np.random.randint(len(y), size=n)
    plt.figure(figsize=(25, 2))
    for i in np.arange(n):
        ax = plt.subplot(1,n,i+1)
        plt.imshow(X[index[i],:,:,:])
        plt.gray()
        ax.set_title(f'{y[index[i]]}-{index[i]}')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()

"""## Build LeNet5 structure

<center><img src="https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/LeNet5.jpg"></center>

En este apartado, pondremos en práctica un punto del nivel medio del desafío: el del uso de la API funcional de tf.keras para la construcción de los modelos.
"""

#
# Build an ANN structure - LeNet5
#

def cnn_model(input_shape):
    #
    # Neural Network Structure
    #
    
    inputs = keras.Input(shape=input_shape)
    x = layers.Conv2D(6, 5, strides=1)(inputs)
    x = layers.Activation("sigmoid")(x)
    x = layers.MaxPool2D(2)(x)
    
    x = layers.Conv2D(16, 5, strides=1)(x)
    x = layers.Activation("sigmoid")(x)
    x = layers.MaxPool2D(2)(x)
    
    x = layers.Flatten()(x)
    x = layers.Dense(120)(x)
    x = layers.Activation("sigmoid")(x)
    
    x = layers.Dense(84)(x)
    x = layers.Activation("sigmoid")(x)
    
    
    x = layers.Dense(nb_classes)(x)
    output = layers.Activation("softmax")(x)
    return keras.Model(inputs,output)

#
# Build an ANN structure - LeNet5
#

def cnn_model_new(input_shape):
    #
    # Neural Network Structure
    #

    inputs = keras.Input(shape=input_shape)
    x = layers.Conv2D(10, 5, strides=1)(inputs)
    x = layers.Activation("sigmoid")(x)
    x = layers.MaxPool2D(2)(x)
    
    x = layers.Conv2D(20, 5, strides=1)(x)
    x = layers.Activation("sigmoid")(x)
    x = layers.MaxPool2D(2)(x)
    
    x = layers.Flatten()(x)
    x = layers.Dense(nb_classes)(x)
    output = layers.Activation("softmax")(x)
    return keras.Model(inputs,output)

"""# Nivel básico

### Calculate AUC
"""

def multiclass_roc_auc_score(y_test, y_pred, average="macro"):
  lb = sklearn.preprocessing.LabelBinarizer()
  lb.fit(y_test)
  y_test = lb.transform(y_test)
  y_pred = lb.transform(y_pred)
  return sklearn.metrics.roc_auc_score(y_test, y_pred, average=average)

"""### Load data"""

##################################################################################
# Main program

X, y = load_data()

print(X.shape, 'train samples')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

"""Only for binary classification. All number of classes greater than 0 will be set to 1."""

if nb_classes==2:
  y[y>0] = 1

"""### Let to see examples of the dataset"""

plot_symbols(X, y, 15)

"""## Clasificación binaria"""

# pd.DataFrame({'ncount':pd.Series(y).value_counts(), '% ncount':pd.Series(y).value_counts(normalize=True)*100})

import collections 

print("Hay ", collections.Counter(y).get(0), " casos de tumor colorrectal")
print("Hay ", collections.Counter(y).get(1), " casos de NO tumor colorrectal")

"""## Split examples in training/test sets

This section is introductory to serve as a simple example. To test the model created in different situations, a 10 cross validation (10-CV) strategy should be used.
"""

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# print(f'X_train {X_train.shape} X_test {X_test.shape}')
# print(f'y_train {y_train.shape} y_test {y_test.shape}')

"""## Validacion cruzada

> Aplicamos validación cruzada 10CV


"""

skf = StratifiedKFold(n_splits=10, shuffle=False)
skf.get_n_splits(X, y)
print(skf)

"""### Validacion cruzada con Adadelta

"""

acc_sum = 0
acc = 0
array_adadelta = []

for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='adadelta', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_adadelta.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Adadelta: ", acc_sum/10.0)
print(array_adadelta)

"""### Validación cruzada con Adagrad"""

acc_sum = 0
acc = 0
array_adagrad = []

for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer="Adagrad", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_adagrad.append(auc)
  print("AUC: " + str(auc))

print("Avg accuracy with Adagrad: ", acc_sum/10.0)
print(array_adagrad)

"""### Validación cruzada con Adam"""

acc_sum = 0
acc = 0
array_adam = []

for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer="Adam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_adam.append(auc)
  print("AUC: " + str(auc))

print("Avg accuracy with Adam: ", acc_sum/10.0)
print(array_adam)

"""### Validación cruzada con Adamax"""

acc_sum = 0
acc = 0
array_adamax = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='adamax', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_adamax.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Adamax: ", acc_sum/10.0)
print(array_adamax)

"""### Validación cruzada con Ftrl"""

acc_sum = 0
acc = 0
array_ftrl = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='ftrl', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_ftrl.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Ftrl: ", acc_sum/10.0)
print(array_ftrl)

"""### Validación cruzada con Nadam"""

acc_sum = 0
acc = 0
array_nadam = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam: ", acc_sum/10.0)
print(array_nadam)

"""### Validación cruzada con RMSprop"""

acc_sum = 0
acc = 0
array_rmsprop = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_rmsprop.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with RMSprop: ", acc_sum/10.0)
print(array_rmsprop)

"""### Validación cruzada con SGD"""

acc_sum = 0
acc = 0
array_sgd = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_sgd.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with SGD: ", acc_sum/10.0)
print(array_sgd)

"""### Representación de los datos"""

plt.figure(0)  
plt.plot(range(len(array_adadelta)),array_adadelta, 'blue', label='Adadelta')
plt.plot(range(len(array_adagrad)),array_adagrad, 'grey', label='Adagrad')
plt.plot(range(len(array_adam)),array_adam, 'red', label='Adam')
plt.plot(range(len(array_adamax)),array_adamax, 'black', label='Adamax') 
plt.plot(range(len(array_ftrl)),array_ftrl, 'brown', label='Ftrl') 
plt.plot(range(len(array_nadam)),array_nadam, 'purple', label='Nadam')
plt.plot(range(len(array_rmsprop)),array_rmsprop, 'green', label='RMSprop') 
plt.plot(range(len(array_sgd)),array_sgd, 'orange', label='SGD')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.4,1)
plt.legend()

plt.show()

"""## Wilcoxon

### Primera prueba
"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

array_adam = [0.7363013698630136, 0.851708646339667, 0.6855575195168655, 0.7465753424657534, 0.8273678008543233, 0.7584722676255857, 0.8423413606479968, 0.7876393883258873, 0.8037303403436127, 0.8274672187715666]
array_adamax = [0.7421564295183385, 0.5715127411989983, 0.5681617322138753, 0.6556562085726911, 0.6625791721903078, 0.6902582543314809, 0.5190875740074825, 0.5158730158730158, 0.7390214667102539, 0.6833932657731284]
array_nadam = [0.85505965532479, 0.8343644130210636, 0.8549860067756665, 0.9119531595227576, 0.6970466931801443, 0.8705822527332826, 0.8650067196977953, 0.6700265155642731, 0.7675529403218191, 0.8286840289128619]
array_rmsprop = [0.89103697157166, 0.5841434673736927, 0.6924068345853587, 0.6532258064516129, 0.8596258653704522, 0.6869710508154444, 0.7879299698521667, 0.7753441574951874, 0.8541280738077077, 0.8609930623660601]

### ADAM
print('Resultados ADAM')

wilcox_W, p_value =  wilcoxon(array_adam, array_adamax, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_adam, array_nadam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_adam, array_rmsprop, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y RMSprop')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### ADAMAX
print('Resultados ADAMAX')

wilcox_W, p_value =  wilcoxon(array_adamax, array_adam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_adamax, array_nadam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_adamax, array_rmsprop, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y RMSprop')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### NADAM
print('Resultados NADAM')

wilcox_W, p_value =  wilcoxon(array_nadam, array_adam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_nadam, array_adamax, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_nadam, array_rmsprop, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y RMSprop')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### RMSPROP
print('Resultados RMSPROP')

wilcox_W, p_value =  wilcoxon(array_rmsprop, array_adam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre RMSprop y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_rmsprop, array_adamax, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre RMSprop y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_rmsprop, array_nadam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre RMSprop y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""### Segunda prueba

#### Adam segunda prueba
"""

acc_sum = 0
acc = 0
array_adam_segunda_prueba = []

for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer="Adam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_adam_segunda_prueba.append(auc)
  print("AUC: " + str(auc))

print("Avg accuracy with Adam: ", acc_sum/10.0)
print(array_adam_segunda_prueba)

"""#### Nadam segunda prueba"""

acc_sum = 0
acc = 0
array_nadam_segunda_prueba = []
for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam_segunda_prueba.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam: ", acc_sum/10.0)
print(array_nadam_segunda_prueba)

"""#### Representación de los datos"""

array_adam_segunda_prueba = [0.8642657239652379, 0.6590072175578141, 0.8492045956694653, 0.8861761673295036, 0.6959051406687289, 0.8773019505284951, 0.8842395844684174, 0.5918056009589191, 0.8296102575278776, 0.7595437870037413]
array_nadam_segunda_prueba = [0.7120341729267934, 0.777839151568714, 0.8770437472381793, 0.9207909854175873, 0.7731256444248048, 0.8196033562166286, 0.8423413606479968, 0.7414550869928445, 0.8759398496240602, 0.8002978460644363]

plt.figure(0)  
plt.plot(range(len(array_adam)),array_adam_segunda_prueba, 'red', label='Adam') 
plt.plot(range(len(array_nadam)),array_nadam_segunda_prueba, 'purple', label='Nadam')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.4,1)
plt.legend()

plt.show()

"""#### Wilcoxon segunda prueba"""

array_adam_segunda_prueba = [0.8642657239652379, 0.6590072175578141, 0.8492045956694653, 0.8861761673295036, 0.6959051406687289, 0.8773019505284951, 0.8842395844684174, 0.5918056009589191, 0.8296102575278776, 0.7595437870037413]
array_nadam_segunda_prueba = [0.7120341729267934, 0.777839151568714, 0.8770437472381793, 0.9207909854175873, 0.7731256444248048, 0.8196033562166286, 0.8423413606479968, 0.7414550869928445, 0.8759398496240602, 0.8002978460644363]

### ADAM
print('Resultados ADAM')

wilcox_W, p_value =  wilcoxon(array_adam, array_nadam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### NADAM
print('Resultados NADAM')

wilcox_W, p_value =  wilcoxon(array_nadam, array_adam, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""## Técnicas de aumento de datos como ImageDataGenerator

Esta técnica es bastante interesante, dado que está claro que tenemos un claro desajuste en los datos que tenemos teniendo en cuenta la clasificación binaria, ya que hay solo 625 casos de tumor colorrectal y 4375 casos de no tumor colorrectal. Vamos a aumentar los datos del caso de tumor colorrectal.
"""

def load_data_generator(name="colorectal_histology"):
  X_1, y_1 = load_data()
  for i in range(0, X_1.shape[0]):
    X_img.append(X_1[i])
    y_label.append(y_1[i])
  return np.array(X_img), np.array(y_label)

"""### Primer aumentado de datos"""

X, y = load_data()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(rotation_range=10)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array,y_to_array,batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()
print(new_X.shape, 'train samples')
print(new_y.shape, 'new_y')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

if nb_classes==2:
  new_y[new_y>0] = 1

tumor_cases = 0
no_tumor_cases = 0
for i in new_y:
  if(i == 0):
    tumor_cases += 1
  else:
    no_tumor_cases += 1

print("Hay ", tumor_cases, " casos de tumor colorrectal")
print("Hay ", no_tumor_cases, " casos de NO tumor colorrectal")

plot_symbols(new_X, new_y, 15)

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(new_X, new_y)
print(skf)

acc_sum = 0;
acc = 0;
array_nadam_data_ampliado = []
for fold, (train_index, test_index) in enumerate(skf.split(new_X, new_y)):
  print("########################### Fold", fold, "/", 10," ###########################")
  X_train, X_test = new_X[train_index], new_X[test_index]
  y_train, y_test = new_y[train_index], new_y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam_data_ampliado.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam and Data Amplied (Rotation): ", acc_sum/10.0)
print(array_nadam_data_ampliado)

"""### Segundo aumentado de datos"""

X, y = load_data()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(zoom_range=0.2)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array, y_to_array, batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()
print(new_X.shape, 'train samples')
print(new_y.shape, 'new_y')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

if nb_classes==2:
  new_y[new_y>0] = 1

tumor_cases = 0
no_tumor_cases = 0
for i in new_y:
  if(i == 0):
    tumor_cases += 1
  else:
    no_tumor_cases += 1

print("Hay ", tumor_cases, " casos de tumor colorrectal")
print("Hay ", no_tumor_cases, " casos de NO tumor colorrectal")

plot_symbols(new_X, new_y, 15)

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(new_X, new_y)
print(skf)

acc_sum = 0
acc = 0
array_nadam_data_ampliado_2 = []
for fold, (train_index, test_index) in enumerate(skf.split(new_X, new_y)):
  print("########################### Fold", fold, "/", 10," ###########################")
  X_train, X_test = new_X[train_index], new_X[test_index]
  y_train, y_test = new_y[train_index], new_y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam_data_ampliado_2.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam and Data Amplied (Zoom range): ", acc_sum/10.0)
print(array_nadam_data_ampliado_2)

"""### Tercer aumentado de datos"""

X, y = load_data()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(horizontal_flip=True)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array, y_to_array, batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()
print(new_X.shape, 'train samples')
print(new_y.shape, 'new_y')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

if nb_classes==2:
  new_y[new_y>0] = 1

tumor_cases = 0
no_tumor_cases = 0
for i in new_y:
  if(i == 0):
    tumor_cases += 1
  else:
    no_tumor_cases += 1

print("Hay ", tumor_cases, " casos de tumor colorrectal")
print("Hay ", no_tumor_cases, " casos de NO tumor colorrectal")

plot_symbols(new_X, new_y, 15)

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(new_X, new_y)
print(skf)

acc_sum = 0
acc = 0
array_nadam_data_ampliado_3 = []
for fold, (train_index, test_index) in enumerate(skf.split(new_X, new_y)):
  print("########################### Fold", fold, "/", 10," ###########################")
  X_train, X_test = new_X[train_index], new_X[test_index]
  y_train, y_test = new_y[train_index], new_y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam_data_ampliado_3.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam and Data Amplied (Horizontal Flip): ", acc_sum/10.0)
print(array_nadam_data_ampliado_3)

"""### Representación de los datos

#### Comparación normal
"""

array_nadam_data_ampliado = [0.940529554977378, 0.9622556241706112, 0.9382751846859554, 0.9267839043708139, 0.9439568247599344, 0.9406026979300545, 0.9794259323114218, 0.9360599981191812, 0.9325569731356383, 0.8754453883368337]
array_nadam_data_ampliado_2 = [0.9496802608068713, 0.9485282593022161, 0.9485883410133434, 0.9599829681410196, 0.9473840945424907, 0.9599855803893296, 0.9588805993542522, 0.960016927369048, 0.9371440811677794, 0.8524602154582406]
array_nadam_data_ampliado_3 = [0.9325439118940889, 0.9154650324441239, 0.9199737730269689, 0.9131061722203065, 0.9096371064647921, 0.9223352454990962, 0.8903900609176305, 0.9119802931987503, 0.9383065316656741, 0.9326039936052162]

plt.figure(0)  
plt.plot(range(len(array_adam)),array_nadam_data_ampliado, 'red', label='Rotación') 
plt.plot(range(len(array_nadam)),array_nadam_data_ampliado_2, 'purple', label='Zoom')
plt.plot(range(len(array_nadam)),array_nadam_data_ampliado_3, 'black', label='Flip horizontal')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.8,1)
plt.legend()

plt.show()

"""#### Comparación con no equilibrado de datos"""

array_nadam_segunda_prueba = [0.7120341729267934, 0.777839151568714, 0.8770437472381793, 0.9207909854175873, 0.7731256444248048, 0.8196033562166286, 0.8423413606479968, 0.7414550869928445, 0.8759398496240602, 0.8002978460644363]
array_nadam_data_ampliado = [0.940529554977378, 0.9622556241706112, 0.9382751846859554, 0.9267839043708139, 0.9439568247599344, 0.9406026979300545, 0.9794259323114218, 0.9360599981191812, 0.9325569731356383, 0.8754453883368337]
array_nadam_data_ampliado_2 = [0.9496802608068713, 0.9485282593022161, 0.9485883410133434, 0.9599829681410196, 0.9473840945424907, 0.9599855803893296, 0.9588805993542522, 0.960016927369048, 0.9371440811677794, 0.8524602154582406]
array_nadam_data_ampliado_3 = [0.9325439118940889, 0.9154650324441239, 0.9199737730269689, 0.9131061722203065, 0.9096371064647921, 0.9223352454990962, 0.8903900609176305, 0.9119802931987503, 0.9383065316656741, 0.9326039936052162]

plt.figure(0)  
plt.plot(range(len(array_adam)),array_nadam_data_ampliado, 'red', label='Rotación') 
plt.plot(range(len(array_nadam)),array_nadam_data_ampliado_2, 'purple', label='Zoom')
plt.plot(range(len(array_nadam)),array_nadam_data_ampliado_3, 'black', label='Flip horizontal')
plt.plot(range(len(array_nadam)),array_nadam_segunda_prueba, 'blue', label='Sin equilibrado')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.6,1)
plt.legend()

plt.show()

"""### Wilcoxon Primero"""

array_nadam_data_ampliado = [0.940529554977378, 0.9622556241706112, 0.9382751846859554, 0.9267839043708139, 0.9439568247599344, 0.9406026979300545, 0.9794259323114218, 0.9360599981191812, 0.9325569731356383, 0.8754453883368337]
array_nadam_data_ampliado_2 = [0.9496802608068713, 0.9485282593022161, 0.9485883410133434, 0.9599829681410196, 0.9473840945424907, 0.9599855803893296, 0.9588805993542522, 0.960016927369048, 0.9371440811677794, 0.8524602154582406]
array_nadam_data_ampliado_3 = [0.9325439118940889, 0.9154650324441239, 0.9199737730269689, 0.9131061722203065, 0.9096371064647921, 0.9223352454990962, 0.8903900609176305, 0.9119802931987503, 0.9383065316656741, 0.9326039936052162]

### Rotación
print('Resultados Rotación')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado, array_nadam_data_ampliado_2, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Rotación y Zoom')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado, array_nadam_data_ampliado_3, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Rotación y Flip horizontal')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### Zoom
print('Resultados Zoom')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado_2, array_nadam_data_ampliado, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Zoom y Rotación')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado_2, array_nadam_data_ampliado_3, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Zoom y Flip horizontal')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### Flip horizontal
print('Resultados Flip horizontal')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado_3, array_nadam_data_ampliado, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Flip horizontal y Rotación')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado_3, array_nadam_data_ampliado_2, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Flip horizontal y Zoom')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""### Wilcoxon segundo"""

array_nadam_data_ampliado = [0.9153474812701795, 0.9313631756580255, 0.9531597755556251, 0.9107655977346583, 0.9577076998631182, 0.9417703729245688, 0.9451950304588153, 0.9554637785649353, 0.9359842429181948, 0.8662372130445232]
array_nadam_data_ampliado_2 = [0.9599829681410196, 0.9588492523745338, 0.9336488929291662, 0.9336567296740959, 0.941660658495554, 0.9383404908937024, 0.969151959708682, 0.9496802608068714, 0.8798783737186922, 0.850187559428649]

### Rotación
print('Resultados Rotación')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado, array_nadam_data_ampliado_2, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Rotación y Zoom')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

### Zoom
print('Resultados Zoom')

wilcox_W, p_value =  wilcoxon(array_nadam_data_ampliado_2, array_nadam_data_ampliado, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Zoom y Rotación')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""## Predicciones con clasificación binaria

### Sin equilibrio de datos
"""

X, y = load_data()

if nb_classes==2:
  y[y>0] = 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

"""#### Con nadam"""

model = cnn_model(input_shape)
model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_nadam = model.predict(X_test).argmax(axis=1)

print('Predictions with Nadam\n', pd.Series(y_pred_nadam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_nadam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_nadam, target_names=target_names))

"""#### Con adam"""

model = cnn_model(input_shape)
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_adam = model.predict(X_test).argmax(axis=1)

print('Predictions with Adam\n', pd.Series(y_pred_adam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_adam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_adam, target_names=target_names))

"""### Con equilibrado de datos

#### Con zoom
"""

X, y = load_data()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(zoom_range=0.2)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array, y_to_array, batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()

if nb_classes==2:
  new_y[new_y>0] = 1

X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

"""##### Con nadam"""

model = cnn_model(input_shape)

model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred = model.predict(X_test).argmax(axis=1)

print('Predictions with nadam\n', pd.Series(y_pred).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred, target_names=target_names))

"""##### Con adam"""

model = cnn_model(input_shape)

model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred = model.predict(X_test).argmax(axis=1)

print('Predictions with adam\n', pd.Series(y_pred).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred, target_names=target_names))

"""#### Con rotación"""

X, y = load_data()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(rotation_range=10)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array, y_to_array, batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()

if nb_classes==2:
  new_y[new_y>0] = 1

X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

"""##### Con nadam"""

model = cnn_model(input_shape)

model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred = model.predict(X_test).argmax(axis=1)

print('Predictions with nadam\n', pd.Series(y_pred).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred, target_names=target_names))

"""##### Con adam"""

model = cnn_model(input_shape)

model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred = model.predict(X_test).argmax(axis=1)

print('Predictions with adam\n', pd.Series(y_pred).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred, target_names=target_names))

"""# Nivel medio

### Calculate F1
"""

def calculate_f1_score(y_test, y_pred, average="macro"):
  lb = sklearn.preprocessing.LabelBinarizer()
  lb.fit(y_test)
  y_test = lb.transform(y_test)
  y_pred = lb.transform(y_pred)
  return sklearn.metrics.f1_score(y_test, y_pred, average=average)

"""## Clasificación con todas las categorías

Pasaremos a estudiar ahora la clasificación con todas las categorías. Estas categorías son:

*   0: TUMOR
*   1: STROMA
*   2: COMPLEX
*   3: LYMPHO
*   4: DEBRIS
*   5: MUCOSA
*   6: ADIPOSE
*   7: EMPTY
"""

nb_classes = 8
def load_data(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

import collections 

X_all_cases, y_all_cases = load_data()

print("Hay ", collections.Counter(y_all_cases).get(0), " casos de tumor colorrectal")
print("Hay ", collections.Counter(y_all_cases).get(1), " casos de estroma")
print("Hay ", collections.Counter(y_all_cases).get(2), " casos de complex")
print("Hay ", collections.Counter(y_all_cases).get(3), " casos de linfo")
print("Hay ", collections.Counter(y_all_cases).get(4), " casos de debris")
print("Hay ", collections.Counter(y_all_cases).get(5), " casos de mucosa")
print("Hay ", collections.Counter(y_all_cases).get(6), " casos de tejido adiposo")
print("Hay ", collections.Counter(y_all_cases).get(7), " casos de sano")

"""### Modelo base"""

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_all_cases, y_all_cases)
print(skf)

acc_sum = 0
f1_sum = 0
epochs = 50
auc_nadam_all_cases = []
f1_nadam_all_cases = []
for fold, (train_index, test_index) in enumerate(skf.split(X_all_cases, y_all_cases)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X_all_cases[train_index], X_all_cases[test_index]
  y_train, y_test = y_all_cases[train_index], y_all_cases[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer="nadam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  auc_nadam_all_cases.append(auc)
  print("AUC: " + str(auc))

  y_pred_int = model.predict(X_test).argmax(axis=1)
  f1 = calculate_f1_score(y_test, y_pred_int)
  f1_sum = f1_sum + f1
  f1_nadam_all_cases.append(f1)
  print("F1: " + str(f1))

print("Media de accuracy con Nadam: ", acc_sum/10.0)
print(auc_nadam_all_cases)

print("Media de f1 con Nadam: ", f1_sum/10.0)
print(f1_nadam_all_cases)

"""### Modelo con parámetros modificados"""

nb_classes = 8
skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_all_cases, y_all_cases)
print(skf)

acc_sum = 0
f1_sum = 0
epochs = 50
new_auc_nadam_all_cases = []
new_f1_nadam_all_cases = []
for fold, (train_index, test_index) in enumerate(skf.split(X_all_cases, y_all_cases)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X_all_cases[train_index], X_all_cases[test_index]
  y_train, y_test = y_all_cases[train_index], y_all_cases[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model_new(input_shape)
  model.compile(loss='categorical_crossentropy',optimizer="nadam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  new_auc_nadam_all_cases.append(auc)
  print("AUC: " + str(auc))

  y_pred_int = model.predict(X_test).argmax(axis=1)
  f1 = calculate_f1_score(y_test, y_pred_int)
  f1_sum = f1_sum + f1
  new_f1_nadam_all_cases.append(f1)
  print("F1: " + str(f1))

print("Media de accuracy con Nadam: ", acc_sum/10.0)
print(new_auc_nadam_all_cases)

print("Media de f1 con Nadam: ", f1_sum/10.0)
print(new_f1_nadam_all_cases)

"""### Representación de los resultados"""

auc_nadam_all_cases = [0.7642483082742881, 0.7966387016255952, 0.7899157124376388, 0.8035912561781278, 0.8288207551330118, 0.7728933695046662, 0.7771081343438135, 0.8238176952111493, 0.8000155436799778, 0.8125850655990524]
new_auc_nadam_all_cases = [0.8093267623370709, 0.8229160958643286, 0.8297239881381937, 0.8195278242204229, 0.7856571824076255, 0.8053451466785853, 0.8052631823720251, 0.7523325335961215, 0.7534032870558498, 0.8077175250867727]

plt.figure(0)  
plt.plot(range(len(auc_nadam_all_cases)),auc_nadam_all_cases, 'red', label='Ejecución normal') 
plt.plot(range(len(new_auc_nadam_all_cases)),new_auc_nadam_all_cases, 'purple', label='Cambio en red')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.5,1)
plt.legend()

plt.show()

f1_nadam_all_cases = [0.5809967288702642, 0.643382617863888, 0.6177370300633416, 0.6499270052758468, 0.6937500022630593, 0.59810988288365, 0.6079991871797218, 0.6868388091350706, 0.6505750409774683, 0.6695553522714257]
new_f1_nadam_all_cases = [0.6604858695450266, 0.686008055368209, 0.6989789909998281, 0.6816953975471621, 0.6107918980011302, 0.6346487047136554, 0.6312968909803285, 0.5147196049588147, 0.522177617629287, 0.6472349402491209]

plt.figure(0)  
plt.plot(range(len(f1_nadam_all_cases)),f1_nadam_all_cases, 'red', label='Ejecución normal') 
plt.plot(range(len(new_f1_nadam_all_cases)),new_f1_nadam_all_cases, 'purple', label='Cambio en red')
plt.xlabel("Fold")  
plt.ylabel("F1") 
plt.xlim(0,9)
plt.ylim(0.5,1)
plt.legend()

plt.show()

"""## Demás ajustes combinados

### Escalado de la imagen para disminuir tamaño
"""

def format_example_16(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (16, 16))
    return image


def load_data_16(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example_16, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

def format_example_20(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (20, 20))
    return image


def load_data_20(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example_20, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

nb_classes = 8
X_16, y_16 = load_data_16()
X_20, y_20 = load_data_20()
X_32, y_32 = load_data()

"""### Reescalado 16 x 16"""

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_16, y_16)
print(skf)

input_shape_16 = (16, 16, 3)
acc_sum = 0
f1_sum = 0
epochs = 50
auc_nadam_16 = []
f1_nadam_16 = []
for fold, (train_index, test_index) in enumerate(skf.split(X_16, y_16)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X_16[train_index], X_16[test_index]
  y_train, y_test = y_16[train_index], y_16[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model_new(input_shape_16)
  model.compile(loss='categorical_crossentropy',optimizer="nadam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  auc_nadam_16.append(auc)
  print("AUC: " + str(auc))

  y_pred_int = model.predict(X_test).argmax(axis=1)
  f1 = calculate_f1_score(y_test, y_pred_int)
  f1_sum = f1_sum + f1
  f1_nadam_16.append(f1)
  print("F1: " + str(f1))

print("Media de accuracy con Nadam: ", acc_sum/10.0)
print(auc_nadam_16)

print("Media de f1 con Nadam: ", f1_sum/10.0)
print(f1_nadam_16)

"""### Reescalado 20 x 20"""

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(X_20, y_20)
print(skf)

input_shape_20 = (20, 20, 3)
acc_sum = 0
f1_sum = 0
epochs = 50
auc_nadam_20 = []
f1_nadam_20 = []
for fold, (train_index, test_index) in enumerate(skf.split(X_20, y_20)):
  print("########################### Fold", fold, "/", 10,"###########################")
  X_train, X_test = X_20[train_index], X_20[test_index]
  y_train, y_test = y_20[train_index], y_20[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = cnn_model_new(input_shape_20)
  model.compile(loss='categorical_crossentropy',optimizer="nadam", metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=0) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  auc_nadam_20.append(auc)
  print("AUC: " + str(auc))

  y_pred_int = model.predict(X_test).argmax(axis=1)
  f1 = calculate_f1_score(y_test, y_pred_int)
  f1_sum = f1_sum + f1
  f1_nadam_20.append(f1)
  print("F1: " + str(f1))

print("Media de accuracy con Nadam: ", acc_sum/10.0)
print(auc_nadam_20)

print("Media de f1 con Nadam: ", f1_sum/10.0)
print(f1_nadam_20)

"""### Representación de los datos"""

auc_nadam_16 = [0.7828390690682145, 0.8343492275722191, 0.8137479068307091, 0.8139229385023139, 0.8025469527357045, 0.8066425267235409, 0.8246552914648755, 0.8087510520017502, 0.8099873667297923, 0.7964509342403123]
f1_nadam_16 = [0.6120226874986964, 0.7041636973868246, 0.6633187453804751, 0.665394054474346, 0.6446842828525323, 0.6546811664341534, 0.6813075445160851, 0.6451033729242239, 0.6563687131803255, 0.6380386157676368]
auc_nadam_20 = [0.7921940448908331, 0.8207997328380221, 0.8012750441181051, 0.783171824260036, 0.8104556413311965, 0.7965440016053569, 0.8157427922852659, 0.8191277986187844, 0.7719169815087971, 0.7872109043655011]
f1_nadam_20 = [0.6339159360040713, 0.6766508940470759, 0.6421030354411936, 0.6036431359467476, 0.6621268889859613, 0.6397941388935471, 0.6738243573228695, 0.6703155183265084, 0.587850136550013, 0.6140525976274844]
auc_nadam_32 = [0.8093267623370709, 0.8229160958643286, 0.8297239881381937, 0.8195278242204229, 0.7856571824076255, 0.8053451466785853, 0.8052631823720251, 0.7523325335961215, 0.7534032870558498, 0.8077175250867727]
f1_nadam_32 = [0.6604858695450266, 0.686008055368209, 0.6989789909998281, 0.6816953975471621, 0.6107918980011302, 0.6346487047136554, 0.6312968909803285, 0.5147196049588147, 0.522177617629287, 0.6472349402491209]

plt.figure(0)  
plt.plot(range(len(auc_nadam_16)),auc_nadam_16, 'red', label='Dimensión 16') 
plt.plot(range(len(auc_nadam_20)),auc_nadam_20, 'purple', label='Dimensión 20')
plt.plot(range(len(auc_nadam_32)),auc_nadam_32, 'grey', label='Dimensión 32')
plt.xlabel("Fold")  
plt.ylabel("AUC") 
plt.xlim(0,9)
plt.ylim(0.5,1)
plt.legend()

plt.show()

plt.figure(0)  
plt.plot(range(len(f1_nadam_16)),f1_nadam_16, 'red', label='Dimensión 16') 
plt.plot(range(len(f1_nadam_20)),f1_nadam_20, 'purple', label='Dimensión 20')
plt.plot(range(len(f1_nadam_32)),f1_nadam_32, 'grey', label='Dimensión 32')
plt.xlabel("Fold")
plt.ylabel("F1") 
plt.xlim(0,9)
plt.ylim(0.5,1)
plt.legend()

plt.show()

"""### Wilcoxon"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

auc_nadam_16 = [0.7828390690682145, 0.8343492275722191, 0.8137479068307091, 0.8139229385023139, 0.8025469527357045, 0.8066425267235409, 0.8246552914648755, 0.8087510520017502, 0.8099873667297923, 0.7964509342403123]
f1_nadam_16 = [0.6120226874986964, 0.7041636973868246, 0.6633187453804751, 0.665394054474346, 0.6446842828525323, 0.6546811664341534, 0.6813075445160851, 0.6451033729242239, 0.6563687131803255, 0.6380386157676368]
auc_nadam_20 = [0.7921940448908331, 0.8207997328380221, 0.8012750441181051, 0.783171824260036, 0.8104556413311965, 0.7965440016053569, 0.8157427922852659, 0.8191277986187844, 0.7719169815087971, 0.7872109043655011]
f1_nadam_20 = [0.6339159360040713, 0.6766508940470759, 0.6421030354411936, 0.6036431359467476, 0.6621268889859613, 0.6397941388935471, 0.6738243573228695, 0.6703155183265084, 0.587850136550013, 0.6140525976274844]
auc_nadam_32 = [0.8093267623370709, 0.8229160958643286, 0.8297239881381937, 0.8195278242204229, 0.7856571824076255, 0.8053451466785853, 0.8052631823720251, 0.7523325335961215, 0.7534032870558498, 0.8077175250867727]
f1_nadam_32 = [0.6604858695450266, 0.686008055368209, 0.6989789909998281, 0.6816953975471621, 0.6107918980011302, 0.6346487047136554, 0.6312968909803285, 0.5147196049588147, 0.522177617629287, 0.6472349402491209]
auc_ejecucion_base = [0.7642483082742881, 0.7966387016255952, 0.7899157124376388, 0.8035912561781278, 0.8288207551330118, 0.7728933695046662, 0.7771081343438135, 0.8238176952111493, 0.8000155436799778, 0.8125850655990524]
f1_ejecucion_base = [0.5809967288702642, 0.643382617863888, 0.6177370300633416, 0.6499270052758468, 0.6937500022630593, 0.59810988288365, 0.6079991871797218, 0.6868388091350706, 0.6505750409774683, 0.6695553522714257]

print('Métrica AUC')
# Ejecución base

wilcox_W, p_value =  wilcoxon(auc_ejecucion_base, auc_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_ejecucion_base, auc_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_ejecucion_base, auc_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 16 AUC
wilcox_W, p_value =  wilcoxon(auc_nadam_16, auc_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_nadam_16, auc_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_nadam_16, auc_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 20 AUC
wilcox_W, p_value =  wilcoxon(auc_nadam_20, auc_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_nadam_20, auc_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_nadam_20, auc_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 32 AUC

wilcox_W, p_value =  wilcoxon(auc_nadam_32, auc_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')
wilcox_W, p_value =  wilcoxon(auc_nadam_32, auc_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(auc_nadam_32, auc_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}\n')

print('Métrica F1')
# Ejecución base

wilcox_W, p_value =  wilcoxon(f1_ejecucion_base, f1_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_ejecucion_base, f1_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_ejecucion_base, f1_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Ejecución base y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 16 F1
wilcox_W, p_value =  wilcoxon(f1_nadam_16, f1_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_16, f1_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_16, f1_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 16 y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 20 F1

wilcox_W, p_value =  wilcoxon(f1_nadam_20, f1_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_20, f1_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_20, f1_nadam_32, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 20 y Dimensión 32')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

## Dimensión 32 F1
wilcox_W, p_value =  wilcoxon(f1_nadam_32, f1_ejecucion_base, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Ejecución base')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_32, f1_nadam_16, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Dimensión 16')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(f1_nadam_32, f1_nadam_20, alternative='greater', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Dimensión 32 y Dimensión 20')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""## Predicciones"""

X, y = load_data()
nb_classes = 8

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

"""### Ejecución base"""

model = cnn_model(input_shape)
model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_nadam = model.predict(X_test).argmax(axis=1)

print('Predictions with Nadam\n', pd.Series(y_pred_nadam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_nadam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_nadam, target_names=target_names))

"""### Dimensión 32 con nuevo modelo"""

model = cnn_model_new(input_shape)
model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_adam = model.predict(X_test).argmax(axis=1)

print('Predictions with Adam\n', pd.Series(y_pred_adam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_adam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_adam, target_names=target_names))

"""### Dimensión 16 con nuevo modelo"""

def format_example_16(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (16, 16))
    return image


def load_data_16(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example_16, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

X_16, y_16 = load_data_16()
nb_classes = 8

X_train, X_test, y_train, y_test = train_test_split(X_16, y_16, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

input_shape_16 = (16, 16, 3)
model = cnn_model_new(input_shape_16)
model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_adam = model.predict(X_test).argmax(axis=1)

print('Predictions with Adam\n', pd.Series(y_pred_adam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_adam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_adam, target_names=target_names))

"""### Dimensión 20 con nuevo modelo"""

def format_example_20(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (20, 20))
    return image


def load_data_20(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example_20, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

X_20, y_20 = load_data_20()
nb_classes = 8

X_train, X_test, y_train, y_test = train_test_split(X_20, y_20, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

input_shape_20 = (20, 20, 3)
model = cnn_model_new(input_shape_20)
model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
model.evaluate(X_test, y_test_nn, batch_size=batch_size)

y_pred_adam = model.predict(X_test).argmax(axis=1)

print('Predictions with Adam\n', pd.Series(y_pred_adam).value_counts(),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred_adam),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred_adam, target_names=target_names))

"""## SHAP (SHapley Additive exPlanations)"""

import shap
import tensorflow.compat.v1.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

X, y = load_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

print(f'x_train {X_train.shape} x_test {X_test.shape}')
print(f'y_train {y_train.shape} y_test {y_test.shape}')
print(f'y_train_nn {y_train_nn.shape} y_test_nn {y_test_nn.shape}')

"""### Sin entreno"""

model = cnn_model(input_shape)
# print(model.summary())
background = X_train[np.random.choice(X_train.shape[0], 100 ,replace = False)]
# print(background)
e = shap.DeepExplainer(model, background)
shap_values = e.shap_values(X_test[1:5])
shap.image_plot(shap_values, X_test[1:5])

"""### Con entreno"""

model.compile(loss='binary_crossentropy',optimizer='nadam', metrics=['accuracy'])
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
print(f'loss: {loss:.2f} acc: {acc:.2f}')

background = X_train[np.random.choice(X_train.shape[0],100,replace = False)]
e = shap.DeepExplainer(model,background)
shap_values = e.shap_values(X_test[1:5])
shap.image_plot(shap_values,X_test[1:5])

"""# Nivel avanzado

## Finetunning

### Red pre-entrenada
"""

from keras.models import Model
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Nadam, Adam
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping

def format_example(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    image = preprocess_input(image)
    return image

def load_data_fine_tunning(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

def load_data_generator(name="colorectal_histology"):
  X_1, y_1 = load_data_fine_tunning()
  for i in range(0, X_1.shape[0]):
    X_img.append(X_1[i])
    y_label.append(y_1[i])
  return np.array(X_img), np.array(y_label)

X, y = load_data_fine_tunning()

X_tumor = []
y_tumor = []
for i in range(0, len(X), 1):
    if(y[i] == 0):
        X_tumor.append(X[i])
        y_tumor.append(y[i])

X_to_array = np.array(X_tumor)
y_to_array = np.array(y_tumor)

datagen = ImageDataGenerator(zoom_range=0.2)

datagen.fit(X_to_array)
X_gen = datagen.flow(X_to_array, y_to_array, batch_size = 1)

X_img = []
y_label = []
for i in range(0, 3750):
    imagen = next(X_gen)[0]
    for im in imagen:
        im = np.asarray(im)
        im = im.squeeze()
        X_img.append(im) 
        y_label.append(0)

new_X, new_y = load_data_generator()
print(new_X.shape, 'train samples')
print(new_y.shape, 'new_y')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

if nb_classes==2:
  new_y[new_y>0] = 1

tumor_cases = 0
no_tumor_cases = 0
for i in new_y:
  if(i == 0):
    tumor_cases += 1
  else:
    no_tumor_cases += 1

print("Hay ", tumor_cases, " casos de tumor colorrectal")
print("Hay ", no_tumor_cases, " casos de NO tumor colorrectal")

def pre_entrenada():
    model = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))

    x = model.output
    x = GlobalAveragePooling2D()(x)
    
    x = layers.Dense(120)(x)
    x = layers.Activation("sigmoid")(x)
    
    x = layers.Dense(84)(x)
    x = layers.Activation("sigmoid")(x)
    
    x = layers.Dense(nb_classes)(x)
    output = layers.Activation("softmax")(x)
    return keras.Model(model.inputs,output)

skf = StratifiedKFold(n_splits=10)
skf.get_n_splits(new_X, new_y)
print(skf)
# early_stopping = EarlyStopping(monitor='loss',patience=3,verbose=1)

acc_sum = 0
acc = 0
array_nadam_data_ampliado_2 = []
for fold, (train_index, test_index) in enumerate(skf.split(new_X, new_y)):
  print("########################### Fold", fold, "/", 10," ###########################")
  X_train, X_test = new_X[train_index], new_X[test_index]
  y_train, y_test = new_y[train_index], new_y[test_index]

  y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
  y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

  model = pre_entrenada()
  model.compile(loss='categorical_crossentropy',optimizer='nadam', metrics=['accuracy'])
  model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=1, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])
  loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
  print(f'loss: {loss:.2f} acc: {acc:.2f}')

  y_pred_int = model.predict(X_test).argmax(axis=1)
  auc = multiclass_roc_auc_score(y_test, y_pred_int)
  acc_sum = acc_sum + auc
  array_nadam_data_ampliado_2.append(auc)
  print("AUC: " + str(auc))

print("AVG Test Accuracy with Nadam and Data Amplied (Zoom range): ", acc_sum/10.0)
print(array_nadam_data_ampliado_2)

# -*- coding: utf-8 -*-
"""Copia de peer-assessment_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Zhk2uxHF-Lr9q5MI2aiHzL24Qk-IiXR

<center><img src="https://images.twinkl.co.uk/tw1n/image/private/t_630/image_repo/b8/06/t-lf-242-pupil-voice-learning-child-led-learning-peer-assessment-cards_ver_1.jpg" height="100"></center>

# Detección de incoherencias en evaluación por pares

Profesor: Juan Ramón Rico (<juanramonrico@ua.es>)

## Descripción
---
No cabe duda que las redes neuronales han avanzado en tareas donde se usa texto y valores numéricos. Concretametne en las encuestas o en la evaluación por pares es habitual encontrar valores numéricos (libres o sujetos a una escala - Likert) y comentarios de los evaluadores respecto a las secciones evaluadas en forma de texto (feedback).

- Artículo sobre evaluación por pares donde se han usado redes recurrentes entre otras metodologías para detectar incongruencias en las respuestas <https://www.sciencedirect.com/science/article/pii/S0360131519301629?dgcid=author> ha servido como base para esta actividad.
---

# Introducción

El uso de la evaluación por pares para actividades abiertas tiene ventajas tanto para los profesores como para los estudiantes. Los profesores pueden reducir la carga de trabajo del proceso de corrección y los estudiantes logran una mejor comprensión de la materia al evaluar las actividades de sus compañeros. Para facilitar el proceso, es aconsejable proporcionar a los estudiantes una rúbrica sobre la cual realizar la evaluación de sus compañeros; sin embargo, limitarse a proporcionar sólo puntuaciones numéricas es perjudicial, ya que impide proporcionar una retroalimentación valiosa a otros compañeros. Dado que esta evaluación produce dos modalidades de la misma evaluación, a saber, la puntuación numérica y la retroalimentación textual, es posible aplicar técnicas automáticas para detectar inconsistencias en la evaluación, minimizando así la carga de trabajo de los profesores para supervisar todo el proceso.

Esta actividad estará enfocada en solo una parte de la detección de incongruencias que será la predicción de calificación de una sección usando únicamente información textual.

# Conjunto de datos

Los datos que vamos a utilizar para esta se pueden descargar en un archivo tipo CSV donde figura:

- `activity`: es la actividad desarrollada, en este caso 1 o 2. Son actividades independientes y los más lógico es entrenar modelos (redes neuronales) independientes para cada una;
-  `year`:  año de comienzo del curso académico estudiado;
- `group`: con valores de 1 si es de mañana o 2 si es de tarde;
- `evaluator`: identificador interno del evaluador para una actividad concreta (la actividad 1 o la 2, de forma excluyente); 
- `work` : es el identificador del trabajo. La entrega se realizaba mediante una URL por lo que hay ocasiones en la que es privada o no accesible y no se ha podido evaluar.
- `secction`: número de sección que se evalua dentro de cada actividad, o 'grade1', 'grade2' cuando se trata de la nota final del trabajo evaluado.
- `value`: valor numérico comprendido entre 0 y 3. Siendo 0 no realizado o realizado incorrectamente y 3 realizado correctamente. Este número puede tener decimales debido a que corresponde al promedio de los valores de la sección. 
- `feedback`: texto libre correspondiente a las recomendaciones que el evaluador realiza en cada sección. Puede estar en blanco lo que indica que no se realizan comentarios y corresponde a que todo está correcto.

Para esta actividad necesitaremos `activity`, `value` y `feeback`.
"""

import numpy as np
import pandas as pd

data = pd.read_csv('https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz', sep='\t', decimal=',')
data.fillna('', inplace=True) # Reemplazar los valores en blanco por cadena vacías
data = data.sample(frac=1, random_state=123) # Barajar los ejemplos de entrada (filas de la tabla)
display(data.head())
data.dtypes

!pip install shap

"""El atributo `section` contiene los identificadores de las diferentes secciones, así como las calificación final (`grade1` y `grade2` cuyos valores están entre 0 y 10) según la actividad. Esta actividad se tiene que evaluar por separado para la actividad 1 o 2 usando únicamente las secciones numéricas (1,2,3,4,5,6 y 7) cuyos valores oscilan entre 0 y 3."""

np.sort(data['section'].unique())

"""# Visualizar las valoraciones"""

data[data['section'].isin(['1','2','3','4','5','6','7'])][['activity','value']].groupby('activity').hist()

"""Podemos ver como la mayoría de valores son cercanos a 3. No obstante tenemos que predecir cuando este valor va disminuyendo atendiendo a las palabras usadas en el contexto restringido de cada actividad.

# Preprocesar el texto

Es necesario preprocesar el texto para descartar símbolos de puntuación, valorar igualmente a palabras en mayúscula y minúscula y extraeer la raiz de las palabras (lemas) para procesarlas como iguales.

Para ello usaremos bibliotecas de procesamiento de lenguaje natural (PLN).
"""

import nltk
nltk.download('stopwords')
  
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

tokenizer = RegexpTokenizer(r'\w+')
stemmer = SnowballStemmer("spanish",ignore_stopwords=True)
stop_words = set(stopwords.words('spanish'))

preprocessed_feedback = []
for i in data.feedback:
  tokens = [stemmer.stem(word) for word in tokenizer.tokenize(i.lower())]
  preprocessed = [i for i in tokens if not i in stop_words]
  preprocessed_feedback.append(np.array(' '.join(preprocessed)))

data['feedback prep'] = preprocessed_feedback
data['feedback prep'] = data['feedback prep'].astype('str')
data.head()

data.dtypes

"""# Convertir datos de entrenamiento a la forma correcta

En este caso los datos de tipo texto hay transformarlos en secuencias de número que recibirá la red neuronal. 

- En el siguiente enlace <https://www.programcreek.com/python/example/106871/keras.preprocessing.text.Tokenizer> hay varios ejemplos de como convertir el texto que ya tenemos preprocesado a secuencias de números;
- De la secuencia de números hay que aplicar `pad_sequences` como truco para igualar la longitud de todas las secuencias de entreada. Nos facilita la tarea de entrenar un red recurrente con `Keras`.
"""

# Seleccionar la actividad 1 y sus secciones, ya que se evaluan por separado. La actividad 2 se seleccionaría por separado
# new_data = data[(data['activity']==1)]

# X = ... # Valores de new_data['feedback prep'] con el padding aplicado
# y = new_data['value'].values

vocabulary_len1 = 226
vocabulary_len2 = 386

"""# Creando la red neuronal

Se han preprocesado los datos y los hemos convertido al formato correcto. Ahora tenemos que diseñar nuestra red para entrenarla y realizar las predicciones.

En el caso de codificar la entrada como un conjunto de palabras (Bag of Words) tendremos un vector de entrada con todas las palabras posibles del vocabulario. Cada posición del vector representa si está presente la palabra o no, el número de veces que se repite, o el valor TF-IDF correspondiente. La dimensión incial de la red cuando entrena será de `(batch_size, vocabulary_len)`.

Otro caso diferente corresponde a una codificación del la entrada atendiendo a una secuencia. Cada valor (entero) del vector de entrada representa a una palabra y la entrada cumple con estas tres dimensiones `(batch_size, time_steps, seq_len)` que necesita una red recurrente.
"""

import keras
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

"""## Red neuronal con bag of words"""

def cnn_model_bag_of_words(input_shape):
  inputs = tf.keras.Input(shape=(input_shape,), dtype="int32")

  x = layers.Dense(128)(inputs)
  x = layers.Activation("relu")(x)

  x = layers.Dense(64)(x)
  x = layers.Activation("relu")(x)

  x = layers.Dense(32)(x)
  x = layers.Activation("relu")(x)

  outputs = layers.Dense(1)(x)
  return keras.Model(inputs,outputs)

"""# Entrenamiento del modelo

Vamos a dividir el conjunto en entrenamiento y test para comprobar nuestro modelo. Para ello tenemos que elegir un tipo de actividad (1 o 2), sus valores y feedback (preprocesado).
"""

current_activity = 1 # 1 or 2
shape = 36

if current_activity == 2:
  shape = 61

new_data = data[data.activity==current_activity]
# n = int(len(new_data)*0.9)

# X_train, X_test = X[:n], X[n:]
# y_train, y_test = y[:n], y[n:]

tokenizer = Tokenizer()
print(tokenizer.fit_on_texts(new_data['feedback prep']))
sequences = tokenizer.texts_to_sequences(new_data['feedback prep'])
X = pad_sequences(sequences)
y = new_data['value'].values
print(len(X))
print(len(y))

"""### Validación cruzada con bag of words

#### Adadelta
"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adadelta_loss_bag_of_words = []
masks = []
print(shape)
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adadelta")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adadelta_loss_bag_of_words.append(loss)

print(adadelta_loss_bag_of_words)

"""#### Adagrad"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adagrad_loss_bag_of_words = []
masks = []
print(shape)
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adagrad")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adagrad_loss_bag_of_words.append(loss)

print(adagrad_loss_bag_of_words)

"""#### Adam"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adam_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adam")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adam_loss_bag_of_words.append(loss)

print(adam_loss_bag_of_words)

"""#### Adamax"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adamax_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adamax")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adamax_loss_bag_of_words.append(loss)

print(adamax_loss_bag_of_words)

"""#### Ftrl"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
ftrl_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="ftrl")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  ftrl_loss_bag_of_words.append(loss)

print(ftrl_loss_bag_of_words)

"""#### Nadam"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
nadam_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="nadam")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  nadam_loss_bag_of_words.append(loss)

print(nadam_loss_bag_of_words)

"""#### RMSprop"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
rmsprop_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="rmsprop")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  print(f'loss: {loss:.2f}')
  rmsprop_loss_bag_of_words.append(loss)

print(rmsprop_loss_bag_of_words)

"""#### SGD"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
sgd_loss_bag_of_words = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="sgd")
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0, callbacks=[callback])
  print(f'loss: {loss:.2f}')
  sgd_loss_bag_of_words.append(loss)

print(sgd_loss_bag_of_words)

"""### Resultados para la primera actividad con bag of world"""

adadelta_loss_bag_of_words = [12.203564643859863, 10.723603248596191, 6.570552825927734, 9.586572647094727, 10.559264183044434, 12.374014854431152, 9.893959045410156, 11.761850357055664, 10.974870681762695, 12.041304588317871]
adagrad_loss_bag_of_words = [3.2268733978271484, 3.3891079425811768, 3.2487707138061523, 3.343717336654663, 3.317692995071411, 3.3157947063446045, 3.370642900466919, 3.369466781616211, 3.6018571853637695, 3.7215123176574707]
adam_loss_bag_of_words = [1.5322670936584473, 1.716211199760437, 1.5897586345672607, 1.5343605279922485, 1.6891461610794067, 1.6817352771759033, 1.715415596961975, 1.5358004570007324, 1.733883023262024, 1.6369287967681885]
adamax_loss_bag_of_words = [1.783388376235962, 1.8722491264343262, 1.7887378931045532, 1.6766388416290283, 1.6583423614501953, 1.759488582611084, 1.8797887563705444, 1.6340434551239014, 1.874679446220398, 1.7721123695373535]
ftrl_loss_bag_of_words = [2.9091556072235107, 3.0839598178863525, 2.973393201828003, 2.8157637119293213, 2.9351086616516113, 3.00488543510437, 3.039269208908081, 2.783482313156128, 3.0436434745788574, 3.0487802028656006]
nadam_loss_bag_of_words = [1.933934211730957, 1.850895643234253, 1.7707685232162476, 1.794068455696106, 1.9842023849487305, 2.5135414600372314, 2.04095196723938, 1.5387271642684937, 1.8551691770553589, 1.833634853363037]
rmsprop_loss_bag_of_words = [1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857]
sgd_loss_bag_of_words = [1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857, 1.462384581565857]

plt.figure(0)  
plt.plot(range(len(adadelta_loss_bag_of_words)),adadelta_loss_bag_of_words, 'blue', label='Adadelta')
plt.plot(range(len(adagrad_loss_bag_of_words)),adagrad_loss_bag_of_words, 'grey', label='Adagrad')
plt.plot(range(len(adam_loss_bag_of_words)),adam_loss_bag_of_words, 'red', label='Adam')
plt.plot(range(len(adamax_loss_bag_of_words)),adamax_loss_bag_of_words, 'black', label='Adamax') 
plt.plot(range(len(ftrl_loss_bag_of_words)),ftrl_loss_bag_of_words, 'brown', label='Ftrl') 
plt.plot(range(len(nadam_loss_bag_of_words)),nadam_loss_bag_of_words, 'purple', label='Nadam')
plt.plot(range(len(rmsprop_loss_bag_of_words)),rmsprop_loss_bag_of_words, 'green', label='RMSprop') 
plt.plot(range(len(sgd_loss_bag_of_words)),sgd_loss_bag_of_words, 'orange', label='SGD')
plt.xlabel("Fold")  
plt.ylabel("MAE") 
plt.xlim(0,9)
plt.ylim(0,17)
plt.legend()

plt.show()

"""### Resultados para la segunda actividad con bag of world"""

adadelta_loss_bag_of_words = [5.126642227172852, 16.2215518951416, 7.824913024902344, 10.118578910827637, 11.47868537902832, 8.10297966003418, 7.0398173332214355, 8.200005531311035, 6.869776248931885, 6.8874125480651855]
adagrad_loss_bag_of_words = [3.325880527496338, 3.1763663291931152, 3.2248144149780273, 3.054781913757324, 3.226881265640259, 3.1209676265716553, 3.2558813095092773, 3.3032803535461426, 3.1278843879699707, 3.342778444290161]
adam_loss_bag_of_words = [0.7333620190620422, 0.6332669854164124, 0.7311221957206726, 0.7215300798416138, 0.5730524659156799, 0.8168253898620605, 0.7312249541282654, 0.8162481188774109, 0.7762805819511414, 0.7428007125854492]
adamax_loss_bag_of_words = [1.4693695306777954, 1.5744107961654663, 1.3421448469161987, 1.4731180667877197, 1.5527737140655518, 1.3625751733779907, 1.7525142431259155, 1.3691222667694092, 1.409522294998169, 1.4302000999450684]
ftrl_loss_bag_of_words = [2.7316925525665283, 2.5806884765625, 2.6243793964385986, 2.5570952892303467, 2.6051669120788574, 2.7941315174102783, 2.677623987197876, 2.5712413787841797, 2.8083341121673584, 2.8395915031433105]
nadam_loss_bag_of_words = [0.6562092304229736, 0.7407015562057495, 0.7791164517402649, 0.750213086605072, 0.7723440527915955, 0.7697961330413818, 0.7583664059638977, 0.7373139262199402, 0.6454000473022461, 0.7226259708404541]
rmsprop_loss_bag_of_words = [0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541]
sgd_loss_bag_of_words = [0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541, 0.7226259708404541]

plt.figure(0)  
plt.plot(range(len(adadelta_loss_bag_of_words)),adadelta_loss_bag_of_words, 'blue', label='Adadelta')
plt.plot(range(len(adagrad_loss_bag_of_words)),adagrad_loss_bag_of_words, 'grey', label='Adagrad')
plt.plot(range(len(adam_loss_bag_of_words)),adam_loss_bag_of_words, 'red', label='Adam')
plt.plot(range(len(adamax_loss_bag_of_words)),adamax_loss_bag_of_words, 'black', label='Adamax') 
plt.plot(range(len(ftrl_loss_bag_of_words)),ftrl_loss_bag_of_words, 'brown', label='Ftrl') 
plt.plot(range(len(nadam_loss_bag_of_words)),nadam_loss_bag_of_words, 'purple', label='Nadam')
plt.plot(range(len(rmsprop_loss_bag_of_words)),rmsprop_loss_bag_of_words, 'green', label='RMSprop') 
plt.plot(range(len(sgd_loss_bag_of_words)),sgd_loss_bag_of_words, 'orange', label='SGD')
plt.xlabel("Fold")  
plt.ylabel("MAE") 
plt.xlim(0,9)
plt.ylim(0,17)
plt.legend()

plt.show()

"""# Test de Wilcoxon

## Primera actividad
"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

adam_loss_bag_of_words = [1.5322670936584473, 1.716211199760437, 1.5897586345672607, 1.5343605279922485, 1.6891461610794067, 1.6817352771759033, 1.715415596961975, 1.5358004570007324, 1.733883023262024, 1.6369287967681885]
adamax_loss_bag_of_words = [1.783388376235962, 1.8722491264343262, 1.7887378931045532, 1.6766388416290283, 1.6583423614501953, 1.759488582611084, 1.8797887563705444, 1.6340434551239014, 1.874679446220398, 1.7721123695373535]
nadam_loss_bag_of_words = [1.933934211730957, 1.850895643234253, 1.7707685232162476, 1.794068455696106, 1.9842023849487305, 2.5135414600372314, 2.04095196723938, 1.5387271642684937, 1.8551691770553589, 1.833634853363037]

print('Wilcoxon para Adam')

wilcox_W, p_value =  wilcoxon(adam_loss_bag_of_words, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(adam_loss_bag_of_words, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Adamax')
wilcox_W, p_value =  wilcoxon(adamax_loss_bag_of_words, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(adamax_loss_bag_of_words, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Nadam')
wilcox_W, p_value =  wilcoxon(nadam_loss_bag_of_words, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(nadam_loss_bag_of_words, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""## Segunda actividad"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

adam_loss_bag_of_words = [0.7333620190620422, 0.6332669854164124, 0.7311221957206726, 0.7215300798416138, 0.5730524659156799, 0.8168253898620605, 0.7312249541282654, 0.8162481188774109, 0.7762805819511414, 0.7428007125854492]
adamax_loss_bag_of_words = [1.4693695306777954, 1.5744107961654663, 1.3421448469161987, 1.4731180667877197, 1.5527737140655518, 1.3625751733779907, 1.7525142431259155, 1.3691222667694092, 1.409522294998169, 1.4302000999450684]
nadam_loss_bag_of_words = [0.6562092304229736, 0.7407015562057495, 0.7791164517402649, 0.750213086605072, 0.7723440527915955, 0.7697961330413818, 0.7583664059638977, 0.7373139262199402, 0.6454000473022461, 0.7226259708404541]

print('Wilcoxon para Adam')

wilcox_W, p_value =  wilcoxon(adam_loss_bag_of_words, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(adam_loss_bag_of_words, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Adamax')
wilcox_W, p_value =  wilcoxon(adamax_loss_bag_of_words, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(adamax_loss_bag_of_words, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Nadam')
wilcox_W, p_value =  wilcoxon(nadam_loss_bag_of_words, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

wilcox_W, p_value =  wilcoxon(nadam_loss_bag_of_words, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""# Ajuste del algoritmo de optimización

Teniendo en cuenta que hemos entrenado la red con este ajuste del algoritmo de optimización (Early Stopping), vamos a entrenar la red con los optimizadores Adam, Adamax y Nadam sin Early Stopping y a comparar los resultados.

## Primera actividad

### Adam
"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adam_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adam")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adam_no_early.append(loss)

print(adam_no_early)

"""### Adamax"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adamax_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adamax")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adamax_no_early.append(loss)

print(adamax_no_early)

"""### Nadam"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
nadam_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="nadam")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  nadam_no_early.append(loss)

print(nadam_no_early)

"""### Test de Wilcoxon"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

adam_loss_bag_of_words = [1.5322670936584473, 1.716211199760437, 1.5897586345672607, 1.5343605279922485, 1.6891461610794067, 1.6817352771759033, 1.715415596961975, 1.5358004570007324, 1.733883023262024, 1.6369287967681885]
adamax_loss_bag_of_words = [1.783388376235962, 1.8722491264343262, 1.7887378931045532, 1.6766388416290283, 1.6583423614501953, 1.759488582611084, 1.8797887563705444, 1.6340434551239014, 1.874679446220398, 1.7721123695373535]
nadam_loss_bag_of_words = [1.933934211730957, 1.850895643234253, 1.7707685232162476, 1.794068455696106, 1.9842023849487305, 2.5135414600372314, 2.04095196723938, 1.5387271642684937, 1.8551691770553589, 1.833634853363037]

adam_no_early = [1.4438177949503848, 1.6904501630548845, 1.6647032733549152, 1.4566231344875535, 1.4914415685754074, 1.6772687217645479, 1.509391440843281, 1.627799769870022, 1.5570674979895875, 1.6070975027586285]
adamax_no_early = [1.696051139580576, 1.921018816295423, 1.6836303997458073, 1.721273702905889, 1.814616798936275, 1.8741480969546134, 1.6372781962679144, 1.5046641399985865, 1.9350678535929897, 1.866671861682022]
nadam_no_early = [1.8408541503705476, 1.8575950045334666, 1.5616450280473944, 1.4927475686658893, 1.7268447566450689, 1.859590975025244, 1.6422366552185594, 1.4907120792489303, 1.570365483719006, 1.5752673763977854]

print('Wilcoxon para Adam')

wilcox_W, p_value =  wilcoxon(adam_no_early, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam sin early y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')
###################################################################################

print('Wilcoxon para Adamax')
wilcox_W, p_value =  wilcoxon(adamax_no_early, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax sin early y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Nadam')
wilcox_W, p_value =  wilcoxon(nadam_no_early, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam sin early y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""## Segunda actividad

### Adam
"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adam_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adam")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adam_no_early.append(loss)

print(adam_no_early)

"""### Adamax"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
adamax_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="adamax")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  adamax_no_early.append(loss)

print(adamax_no_early)

"""### Nadam"""

k = 10
n_samples = len(X)
fold_size = n_samples // k
nadam_no_early = []
masks = []
for fold in range(k):
  print("########################### Fold", fold, "/", 10,"###########################")
  print(fold * fold_size)
  print((fold + 1) * fold_size)

  test_mask = np.zeros(n_samples, dtype=bool)
  test_mask[fold * fold_size : (fold + 1) * fold_size] = True
  masks.append(test_mask)

  X_train, y_train = X[~test_mask], y[~test_mask]
  X_test, y_test = X[test_mask], y[test_mask]
  
  model = cnn_model_bag_of_words(shape)
  model.compile(loss="mean_absolute_error", optimizer="nadam")
  model.fit(X_train, y_train, batch_size=256, epochs=25, validation_split=0.1, verbose=0)
  loss = model.evaluate(X_test, y_test, batch_size=32)
  print(f'loss: {loss:.2f}')
  nadam_no_early.append(loss)

print(nadam_no_early)

"""### Test de Wilcoxon"""

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import wilcoxon

adam_loss_bag_of_words = [0.7333620190620422, 0.6332669854164124, 0.7311221957206726, 0.7215300798416138, 0.5730524659156799, 0.8168253898620605, 0.7312249541282654, 0.8162481188774109, 0.7762805819511414, 0.7428007125854492]
adamax_loss_bag_of_words = [1.4693695306777954, 1.5744107961654663, 1.3421448469161987, 1.4731180667877197, 1.5527737140655518, 1.3625751733779907, 1.7525142431259155, 1.3691222667694092, 1.409522294998169, 1.4302000999450684]
nadam_loss_bag_of_words = [0.6562092304229736, 0.7407015562057495, 0.7791164517402649, 0.750213086605072, 0.7723440527915955, 0.7697961330413818, 0.7583664059638977, 0.7373139262199402, 0.6454000473022461, 0.7226259708404541]

adam_no_early = [0.6168646812438965, 0.7020475268363953, 0.8394378423690796, 0.6733673214912415, 0.7139591574668884, 0.7117514610290527, 0.6234015226364136, 0.6792083382606506, 0.7092501521110535, 0.7275382876396179]
adamax_no_early = [1.4550669193267822, 1.337406039237976, 1.3380773067474365, 1.2716761827468872, 1.6706942319869995, 1.207666277885437, 1.4138273000717163, 1.3519995212554932, 1.1844488382339478, 1.420169711112976]
nadam_no_early = [0.7776003479957581, 0.7335073947906494, 0.707342267036438, 0.6129120588302612, 0.6639992594718933, 0.7044883966445923, 0.7547983527183533, 0.6984056234359741, 0.779606819152832, 0.8019728660583496]

print('Wilcoxon para Adam')

wilcox_W, p_value =  wilcoxon(adam_no_early, adam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adam sin early y Adam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')
###################################################################################

print('Wilcoxon para Adamax')
wilcox_W, p_value =  wilcoxon(adamax_no_early, adamax_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Adamax sin early y Adamax')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

###################################################################################

print('Wilcoxon para Nadam')
wilcox_W, p_value =  wilcoxon(nadam_no_early, nadam_loss_bag_of_words, alternative='less', zero_method='wilcox', correction=False)
 
print('Resultado completo del test de Wilcoxon entre Nadam sin early y Nadam')
print(f'Wilcox W: {wilcox_W}, p-value: {p_value:.5f}')

"""# SHAP (SHapley Additive exPlanations)"""

import shap
import tensorflow.compat.v1.keras.backend as K
import tensorflow as tf
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
tf.compat.v1.disable_eager_execution()

current_activity = 1 # 1 or 2
shape = 36

if current_activity == 2:
  shape = 61

new_data = data[data.activity==current_activity]
# n = int(len(new_data)*0.9)

# X_train, X_test = X[:n], X[n:]
# y_train, y_test = y[:n], y[n:]

tokenizer = Tokenizer()
print(tokenizer.fit_on_texts(new_data['feedback prep']))
sequences = tokenizer.texts_to_sequences(new_data['feedback prep'])
X = pad_sequences(sequences)
y = new_data['value'].values
print(len(X))
print(len(y))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model = cnn_model_bag_of_words(shape)
# print(model.summary())
# X_test_summary = shap.kmeans(X_test, 150)
explainer = shap.KernelExplainer(model.predict, X_test[1:50])
shap_values = explainer.shap_values(X_test[1:50])

shap.summary_plot(shap_values, plot_type='bar')